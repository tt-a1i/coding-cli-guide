import { Layer } from '../components/Layer';
import { HighlightBox } from '../components/HighlightBox';
import { CodeBlock } from '../components/CodeBlock';
import { MermaidDiagram } from '../components/MermaidDiagram';
import { RelatedPages, type RelatedPage } from '../components/RelatedPages';

const relatedPages: RelatedPage[] = [
  { id: 'token-accounting', label: 'Tokenè®¡è´¹ç³»ç»Ÿ', description: 'é¢„ä¼°ä¸ usageMetadata çš„è¾¹ç•Œ' },
  { id: 'token-lifecycle-overview', label: 'Tokenç”Ÿå‘½å‘¨æœŸå…¨æ™¯', description: 'ä»è¾“å…¥åˆ° Finished çš„å…¨é“¾è·¯' },
  { id: 'token-counting-anim', label: 'Token è®¡æ•°åŠ¨ç”»', description: 'estimateTokenCountSync / countTokens åˆ†æ”¯' },
  { id: 'token-limit-matcher-anim', label: 'Token é™åˆ¶åŒ¹é…', description: 'tokenLimit(model) çš„æ¥æº' },
  { id: 'chat-compression', label: 'èŠå¤©å‹ç¼©ç³»ç»Ÿ', description: 'tryCompressChat å¦‚ä½•å½±å“ä¸Šä¸‹æ–‡' },
  { id: 'retry', label: 'é‡è¯•å›é€€', description: 'InvalidStreamError ä¸é‡è¯•ä¿¡å·' },
];

export function TokenManagementStrategy() {
  const overviewDiagram = `
flowchart TD
  A[å‡†å¤‡å‘é€è¯·æ±‚] --> B[remainingTokenCount = tokenLimit(model) - lastPromptTokenCount]
  B --> C[estimatedRequestTokenCount = calculateRequestTokenCount(request)]
  C --> D{estimatedRequestTokenCount > remainingTokenCount * 0.95?}
  D -- Yes --> E[Yield ContextWindowWillOverflow å¹¶ä¸­æ­¢æœ¬è½®]
  D -- No --> F[tryCompressChat()]
  F --> G[Turn.run(): decode stream â†’ GeminiEventType]
  G --> H[usageMetadata å‡ºç°æ—¶æ›´æ–° lastPromptTokenCount]
  G --> I[Finished(reason + usageMetadata)]
`;

  return (
    <div className="space-y-8 animate-fadeIn">
      <div className="border-b border-[var(--border-subtle)] pb-6">
        <h1 className="text-3xl font-bold text-[var(--text-primary)] mb-2">
          ğŸ“Š Token è®¡ç®—ç­–ç•¥ï¼ˆä¸Šæ¸¸ Gemini CLIï¼‰
        </h1>
        <p className="text-[var(--text-secondary)]">
          æ ¸å¿ƒç›®æ ‡åªæœ‰ä¸€ä¸ªï¼šåœ¨è¯·æ±‚å‘å‡ºå»ä¹‹å‰å°½é‡é¿å…â€œä¸Šä¸‹æ–‡çª—å£æº¢å‡ºâ€ï¼ŒåŒæ—¶æŠŠçœŸå® usage è®°å½•ä¸‹æ¥ç”¨äº UI/é¥æµ‹/ä¼šè¯è®°å½•ã€‚
        </p>
        <div className="mt-4 flex flex-wrap gap-2">
          <span className="px-2 py-1 bg-[var(--terminal-green)]/20 text-[var(--terminal-green)] text-xs rounded">
            æ ¸å¿ƒæœºåˆ¶
          </span>
          <span className="px-2 py-1 bg-[var(--cyber-blue)]/20 text-[var(--cyber-blue)] text-xs rounded">
            gemini-cli/packages/core/src/utils/tokenCalculation.ts
          </span>
          <span className="px-2 py-1 bg-[var(--purple)]/20 text-[var(--purple)] text-xs rounded">
            gemini-cli/packages/core/src/core/client.ts
          </span>
        </div>
      </div>

      <HighlightBox title="âš¡ 30 ç§’é€Ÿè§ˆ" variant="blue">
        <ul className="m-0 leading-relaxed">
          <li><strong>ä¸¤å¥—æ•°å­—</strong>ï¼šé¢„ä¼° tokenï¼ˆæœ¬åœ°ä¼°ç®—/å¯é€‰ APIï¼‰ vs çœŸå® tokenï¼ˆusageMetadataï¼‰</li>
          <li><strong>é¢„è­¦æœºåˆ¶</strong>ï¼šæ¥è¿‘æº¢å‡ºç›´æ¥ yield <code>ContextWindowWillOverflow</code>ï¼Œé¿å… API å¤±è´¥</li>
          <li><strong>ä¼°ç®—ç­–ç•¥</strong>ï¼šçº¯æ–‡æœ¬èµ°å­—ç¬¦å¯å‘å¼ï¼›å¸¦å›¾ç‰‡/æ–‡ä»¶èµ° <code>countTokens</code> APIï¼ˆå¤±è´¥å†é™çº§ï¼‰</li>
          <li><strong>å‹ç¼©ä½ç½®</strong>ï¼šåœ¨é€šè¿‡æº¢å‡ºæ£€æŸ¥åæ‰å°è¯• <code>tryCompressChat</code>ï¼ˆä¼˜åŒ–å†å²ï¼Œä¸æ•‘â€œè¶…å¤§è¯·æ±‚â€ï¼‰</li>
          <li><strong>çœŸå®å¯¹é½</strong>ï¼šä¸€æ—¦å“åº” chunk å¸¦ <code>usageMetadata.promptTokenCount</code>ï¼Œå°±æ›´æ–° <code>lastPromptTokenCount</code></li>
        </ul>
      </HighlightBox>

      <Layer title="å…³é”®æ–‡ä»¶ï¼ˆä¸Šæ¸¸æºç ï¼‰" icon="ğŸ“" defaultOpen>
        <ul className="text-sm text-[var(--text-secondary)] space-y-2">
          <li>
            <code className="bg-black/30 px-1 rounded">gemini-cli/packages/core/src/utils/tokenCalculation.ts</code>ï¼š
            <span className="text-[var(--text-muted)]">estimateTokenCountSync / calculateRequestTokenCount</span>
          </li>
          <li>
            <code className="bg-black/30 px-1 rounded">gemini-cli/packages/core/src/core/tokenLimits.ts</code>ï¼š
            <span className="text-[var(--text-muted)]">tokenLimit(model)ï¼ˆä¸Šä¸‹æ–‡çª—å£ï¼‰</span>
          </li>
          <li>
            <code className="bg-black/30 px-1 rounded">gemini-cli/packages/core/src/core/client.ts</code>ï¼š
            <span className="text-[var(--text-muted)]">processTurn é‡Œçš„æº¢å‡ºæ£€æŸ¥ä¸ ChatCompressed äº‹ä»¶</span>
          </li>
          <li>
            <code className="bg-black/30 px-1 rounded">gemini-cli/packages/core/src/core/geminiChat.ts</code>ï¼š
            <span className="text-[var(--text-muted)]">usageMetadata â†’ lastPromptTokenCount æ›´æ–°ä¸å½•åˆ¶</span>
          </li>
        </ul>
      </Layer>

      <Layer title="ç­–ç•¥æ€»è§ˆï¼šå…ˆç®—å†å‘" icon="ğŸ§­" defaultOpen>
        <p className="text-[var(--text-secondary)] mb-4">
          è¿™å¼ å›¾å¯¹åº”ä¸Šæ¸¸ <code>GeminiClient.processTurn()</code> çš„å…³é”®é¡ºåºï¼šå…ˆç®—å‰©ä½™ç©ºé—´ã€å†ä¼°æœ¬æ¬¡è¯·æ±‚ã€è¿‡çº¿å°±é¢„è­¦å¹¶åœæ­¢ã€‚
        </p>
        <MermaidDiagram chart={overviewDiagram} />
      </Layer>

      <Layer title="é¢„ä¼° 1ï¼šestimateTokenCountSyncï¼ˆå­—ç¬¦å¯å‘å¼ï¼‰" icon="ğŸ§®">
        <p className="text-[var(--text-secondary)] mb-4">
          ä¸Šæ¸¸å¯¹â€œçº¯æ–‡æœ¬/è½»é‡ partâ€ä½¿ç”¨æœ¬åœ°å¯å‘å¼ï¼šASCII çº¦ <code>0.25 token/char</code>ï¼Œé ASCIIï¼ˆå« CJKï¼‰ç”¨æ›´ä¿å®ˆçš„
          <code>1.3 token/char</code>ï¼›éæ–‡æœ¬ partï¼ˆfunctionCall/response ç­‰ï¼‰ç”¨ <code>JSON.length/4</code> ä¼°ç®—ã€‚
        </p>
        <CodeBlock
          language="typescript"
          code={`// gemini-cli/packages/core/src/utils/tokenCalculation.ts
const ASCII_TOKENS_PER_CHAR = 0.25;
const NON_ASCII_TOKENS_PER_CHAR = 1.3;

export function estimateTokenCountSync(parts: Part[]): number {
  let totalTokens = 0;
  for (const part of parts) {
    if (typeof part.text === 'string') {
      for (const char of part.text) {
        totalTokens += char.codePointAt(0)! <= 127
          ? ASCII_TOKENS_PER_CHAR
          : NON_ASCII_TOKENS_PER_CHAR;
      }
    } else {
      totalTokens += JSON.stringify(part).length / 4;
    }
  }
  return Math.floor(totalTokens);
}`}
        />
        <HighlightBox title="ä¸ºä»€ä¹ˆä¸æ˜¯ tiktokenï¼Ÿ" icon="ğŸ’¡" variant="yellow">
          <p className="m-0 text-sm text-[var(--text-secondary)]">
            Gemini CLI çš„ä¸Šæ¸¸ä¸»çº¿ä¸ä¾èµ– OpenAI åˆ†è¯å™¨ï¼›å®ƒéœ€è¦ä¸€ä¸ªâ€œå¿«é€Ÿã€æ— å¤–éƒ¨ä¾èµ–ã€å¯ä¿å®ˆä¼°è®¡â€çš„æ–¹æ¡ˆæ¥åšé¢„è­¦ä¸æµç¨‹æ§åˆ¶ï¼Œ
            ç²¾ç¡® usage åˆ™äº¤ç»™ API è¿”å›çš„ <code>usageMetadata</code>ã€‚
          </p>
        </HighlightBox>
      </Layer>

      <Layer title="é¢„ä¼° 2ï¼šcalculateRequestTokenCountï¼ˆé‡åˆ°åª’ä½“å°±ç”¨ APIï¼‰" icon="ğŸ”€">
        <p className="text-[var(--text-secondary)] mb-4">
          å½“è¯·æ±‚åŒ…å«å›¾ç‰‡/æ–‡ä»¶ç­‰åª’ä½“ partï¼ˆ<code>inlineData</code>/<code>fileData</code>ï¼‰æ—¶ï¼Œæœ¬åœ°å¾ˆéš¾å¯é ä¼°ç®—ï¼Œ
          ä¸Šæ¸¸ä¼šè°ƒç”¨ <code>countTokens</code> APIï¼›å¦‚æœ API å¤±è´¥ï¼Œå†é™çº§ä¸ºæœ¬åœ°å¯å‘å¼ã€‚
        </p>
        <CodeBlock
          language="typescript"
          code={`// gemini-cli/packages/core/src/utils/tokenCalculation.ts
export async function calculateRequestTokenCount(request, contentGenerator, model) {
  const parts = normalizeToParts(request);
  const hasMedia = parts.some((p) => 'inlineData' in p || 'fileData' in p);

  if (hasMedia) {
    try {
      const resp = await contentGenerator.countTokens({
        model,
        contents: [{ role: 'user', parts }],
      });
      return resp.totalTokens ?? 0;
    } catch {
      return estimateTokenCountSync(parts);
    }
  }

  return estimateTokenCountSync(parts);
}`}
        />
      </Layer>

      <Layer title="é¢„è­¦ï¼šContextWindowWillOverflowï¼ˆ95% é˜ˆå€¼ï¼‰" icon="ğŸš¨">
        <p className="text-[var(--text-secondary)] mb-4">
          ä¸Šæ¸¸æŠŠâ€œæº¢å‡ºé£é™©â€ä½œä¸ºä¸€ä¸ª<strong>äº‹ä»¶</strong>æŠ›ç»™ UIï¼Œè€Œä¸æ˜¯èµŒä¸€æŠŠæŠŠè¯·æ±‚å‘å‡ºå»ã€‚
          æ¡ä»¶æ˜¯ <code>estimatedRequestTokenCount &gt; remainingTokenCount * 0.95</code>ã€‚
        </p>
        <CodeBlock
          language="typescript"
          code={`// gemini-cli/packages/core/src/core/client.ts (simplified)
const estimatedRequestTokenCount = await calculateRequestTokenCount(request, contentGenerator, model);
const remainingTokenCount = tokenLimit(model) - chat.getLastPromptTokenCount();

if (estimatedRequestTokenCount > remainingTokenCount * 0.95) {
  yield {
    type: GeminiEventType.ContextWindowWillOverflow,
    value: { estimatedRequestTokenCount, remainingTokenCount },
  };
  return turn;
}`}
        />
        <HighlightBox title="è¯»è€…è¦è®°ä½çš„ç‚¹" icon="ğŸ“Œ" variant="green">
          <ul className="m-0 text-sm text-[var(--text-secondary)] space-y-1">
            <li>è¿™æ˜¯â€œè¯·æ±‚å‘å‡ºå‰â€çš„ä¿æŠ¤é—¨æ§›ï¼›ä¸æ˜¯æ¨¡å‹è¿”å›çš„ finishReasonã€‚</li>
            <li>å®ƒä¸»è¦ä¿æŠ¤ä¸Šä¸‹æ–‡çª—å£ï¼ˆinput contextï¼‰ï¼Œä¸æ˜¯è¾“å‡ºé•¿åº¦ã€‚</li>
            <li>é€šè¿‡é¢„ä¼°åšæ—©åœï¼šå®å¯ä¿å®ˆæ‹’ç»ï¼Œä¹Ÿä¸è¦è®© API ç›´æ¥æŠ¥é”™ã€‚</li>
          </ul>
        </HighlightBox>
      </Layer>

      <Layer title="å¯¹é½ï¼šusageMetadata æ›´æ–° lastPromptTokenCount" icon="ğŸ§¾">
        <p className="text-[var(--text-secondary)] mb-4">
          ä¸€æ—¦æ¨¡å‹åœ¨æµå¼ chunk é‡Œè¿”å› <code>usageMetadata</code>ï¼Œä¸Šæ¸¸ä¼šæŠŠå®ƒè®°å½•åˆ°ä¼šè¯è®°å½•ï¼Œå¹¶ç”¨
          <code>promptTokenCount</code> æ›´æ–° <code>lastPromptTokenCount</code>ï¼ˆåç»­æº¢å‡ºåˆ¤æ–­æ›´å‡†ï¼‰ã€‚
        </p>
        <CodeBlock
          language="typescript"
          code={`// gemini-cli/packages/core/src/core/geminiChat.ts (simplified)
for await (const chunk of streamResponse) {
  if (chunk.usageMetadata) {
    chatRecordingService.recordMessageTokens(chunk.usageMetadata);
    if (chunk.usageMetadata.promptTokenCount !== undefined) {
      lastPromptTokenCount = chunk.usageMetadata.promptTokenCount;
    }
  }
  yield chunk;
}`}
        />
      </Layer>

      <RelatedPages pages={relatedPages} />
    </div>
  );
}

